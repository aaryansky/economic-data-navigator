{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3defb2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 PDF files to process.\n",
      "Loading file: Annual_Report_RBI_2020-2021.pdf...\n",
      "Loading file: Annual_Report_RBI_2021-2022.pdf...\n",
      "Loading file: Annual_Report_RBI_2022-2023.pdf...\n",
      "Loading file: Annual_Report_RBI_2023-2024.pdf...\n",
      "Loading file: Annual_Report_RBI_2024-2025.pdf...\n",
      "Loading file: Budget_Speech_2020-2021.pdf...\n",
      "Loading file: Budget_Speech_2021-2022.pdf...\n",
      "Loading file: Budget_Speech_2022-2023.pdf...\n",
      "Loading file: Budget_Speech_2023-2024.pdf...\n",
      "Loading file: Budget_Speech_2024-2025.pdf...\n",
      "Loading file: Economic_Survey_2020-2021.pdf...\n",
      "Loading file: Economic_Survey_2022-2023.pdf...\n",
      "Loading file: Economic_Survey_2023-2024.pdf...\n",
      "Loading file: Economic_Survey_2024-2025.pdf...\n",
      "Loading file: Economix_Survey_2021-2022.pdf...\n",
      "\n",
      "Successfully loaded a total of 4148 pages from all files.\n",
      "Split all documents into 13843 chunks.\n",
      "\n",
      "Creating and persisting the new, expanded vector store... This will take a significant amount of time.\n",
      "\n",
      "New vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- 1. FIND ALL PDF FILES ---\n",
    "raw_data_path = '../data/raw/'\n",
    "pdf_files = glob.glob(os.path.join(raw_data_path, '*.pdf'))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"Error: No PDF files found in the '/data/raw/' folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process.\")\n",
    "    \n",
    "    all_documents = []\n",
    "    # --- 2. LOAD ALL DOCUMENTS ---\n",
    "    for file_path in pdf_files:\n",
    "        print(f\"Loading file: {os.path.basename(file_path)}...\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            all_documents.extend(documents)\n",
    "        except Exception as e:\n",
    "            print(f\"--> Error loading {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded a total of {len(all_documents)} pages from all files.\")\n",
    "\n",
    "    # --- 3. SPLIT DOCUMENTS INTO CHUNKS ---\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    splits = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split all documents into {len(splits)} chunks.\")\n",
    "\n",
    "    # --- 4. GENERATE EMBEDDINGS AND CREATE VECTOR STORE ---\n",
    "    # This will use the same local model running on your CPU\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "    persist_directory = '../vector_store'\n",
    "    \n",
    "    # This will overwrite your old vector store with the new, comprehensive one\n",
    "    print(\"\\nCreating and persisting the new, expanded vector store... This will take a significant amount of time.\")\n",
    "    # 1. Create the vector store in memory\n",
    "    vectordb = FAISS.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "    # 2. Save it to the specified folder\n",
    "    vectordb.save_local(folder_path=persist_directory)\n",
    "    \n",
    "    print(\"\\nNew vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a54870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document into 1263 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split the document into chunks\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split the document into {len(splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c295b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and persisting vector store... This will take a while.\n",
      "\n",
      "Creating and persisting the new FAISS vector store...\n",
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Specify the embedding model you want to use\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# We will use the CPU for embedding\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "# Initialize the HuggingFaceEmbeddings model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "# Define the path for the persistent vector store\n",
    "persist_directory = r'C:\\economic_data_navigator\\vector_store'\n",
    "\n",
    "# Create the vector store from the document splits and save it to disk\n",
    "print(\"Creating and persisting vector store... This will take a while.\")\n",
    "print(\"\\nCreating and persisting the new FAISS vector store...\")\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "vectordb.save_local(persist_directory)\n",
    "\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35630a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Most Relevant Chunk ---\n",
      "20 per cent reflecting higher international petroleum prices. Although the high WPI inflation is \n",
      "partly due to base effects that will even out, India does need to be wary of imported inflation, \n",
      "especially from elevated global energy prices.\n",
      "Figure 33: Consumer Price Inflation Rates Figure 34: CPI and WPI Inflation\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "Per cent\n",
      "AEs EMDEs\n",
      "-8%\n",
      "-4%\n",
      "0%\n",
      "4%\n",
      "8%\n",
      "12%\n",
      "16%\n",
      "Apr-20\n",
      "Jun-20\n",
      "Aug-20\n",
      "Oct-20\n",
      "Dec-20\n",
      "Feb-21\n",
      "Apr-21\n",
      "Jun-21\n",
      "Aug-21\n",
      "Oct-21\n",
      "Dec-21\n",
      "CPI WPI\n",
      "Source: World Economic Outlook, January 2022 Update, \n",
      "IMF\n",
      "Note: Figures are annual averages; Figures for 2021 are \n",
      "projections. Advanced Economies include 40 economies \n",
      "and Emerging Markets and Developing Economies \n",
      "(EMDEs) include 156 economies as per IMF classification\n",
      "Source: MoSPI, DPIIT\n",
      "1.38 Overall, macro-econo mic stability indicators suggest that the Indian economy is well-\n",
      "placed to take on the challenges of 2022-23.\n",
      "Box 2: Global Supply-Side Disruption\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load the persisted vector store from disk using the correct method\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path=persist_directory,\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for loading local FAISS indexes\n",
    ")\n",
    "\n",
    "# Define a test query\n",
    "query = \"What is the outlook for the Indian economy in FY26?\"\n",
    "\n",
    "# Perform a similarity search (this part remains the same)\n",
    "retrieved_docs = vectordb.similarity_search(query, k=3)\n",
    "\n",
    "# Print the content of the most relevant chunk\n",
    "print(\"--- Most Relevant Chunk ---\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6ed051",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# --- 1. SET UP YOUR API KEY ---\n",
    "# IMPORTANT: Replace \"YOUR_API_KEY\" with the key you just created.\n",
    "# For better security, it's best to set this as an environment variable.\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyDytLcfHpglegmavoKuF8YJvJjTQRYh-1I\"\n",
    "\n",
    "\n",
    "# --- 2. LOAD YOUR EXISTING VECTOR STORE & EMBEDDINGS ---\n",
    "# (This is the same code as your test step from before)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "persist_directory = '../vector_store'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "\n",
    "# --- 3. INITIALIZE THE LLM (GEMINI) ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "\n",
    "# --- 4. CREATE A PROMPT TEMPLATE ---\n",
    "# This template instructs the LLM to answer the question based ONLY on the provided context.\n",
    "prompt_template = \"\"\"\n",
    "Answer the user's question based only on the following context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "# --- 5. CREATE THE RAG CHAIN ---\n",
    "# First, create a chain to combine the documents into a single prompt (\"stuff\" chain)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Now, create the main retrieval chain\n",
    "# This chain takes a question, retrieves documents, and then passes them to the document_chain\n",
    "retriever = vectordb.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "\n",
    "# --- 6. INVOKE THE CHAIN AND GET AN ANSWER ---\n",
    "query = \"What is the outlook for the Indian economy in FY26?\"\n",
    "\n",
    "# The invoke method runs the entire chain and returns a dictionary\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "# Let's see the full response dictionary\n",
    "print(\"--- Full Response Dictionary ---\")\n",
    "print(response)\n",
    "\n",
    "# Now, let's print just the clean answer\n",
    "print(\"\\n--- Generated Answer ---\")\n",
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
