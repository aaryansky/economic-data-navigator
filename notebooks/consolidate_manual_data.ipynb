{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09338d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files to process.\n",
      "Successfully read and processed table from ../data/processed\\Press Release for auction of G-Sec- 11.08.2025.csv\n",
      "Successfully read and processed table from ../data/processed\\Press Release for auction of G-Sec- 21.07.2025.csv\n",
      "Successfully read and processed table from ../data/processed\\Press Release for auction of G-Sec- 28.07.2025.csv\n",
      "\n",
      "--- Preview of Consolidated Data ---\n",
      "             name_of_the_security    date_of_issue      date_of_maturity  \\\n",
      "0  6.01% Government Security 2030  August 14, 2025         June 22, 2030   \n",
      "1    New Government Security 2055  August 14, 2025  To be notified later   \n",
      "2  7.35% Government Security 2029    July 22, 2025         June 22, 2029   \n",
      "3  7.26% Government Security 2033    July 22, 2025         July 22, 2033   \n",
      "4    New Government Security 2035    July 22, 2025  To be notified later   \n",
      "\n",
      "            coupon_rate notified_amount_(₹_crore)  \\\n",
      "0                 6.01%                    15,000   \n",
      "1  To be notified later                    13,000   \n",
      "2                 7.35%                    12,000   \n",
      "3                 7.26%                     7,000   \n",
      "4  To be notified later                     8,000   \n",
      "\n",
      "                                         source_file  \n",
      "0  Press Release for auction of G-Sec- 11.08.2025...  \n",
      "1  Press Release for auction of G-Sec- 11.08.2025...  \n",
      "2  Press Release for auction of G-Sec- 21.07.2025...  \n",
      "3  Press Release for auction of G-Sec- 21.07.2025...  \n",
      "4  Press Release for auction of G-Sec- 21.07.2025...  \n",
      "\n",
      "Successfully saved consolidated data to ../data/processed/consolidated_gsec_auctions_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to your processed data folder\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# Find all the CSV files starting with \"Press Release\"\n",
    "files = glob.glob(os.path.join(processed_data_path, 'Press Release*.csv'))\n",
    "\n",
    "if not files:\n",
    "    print(\"Error: No CSV files starting with 'Press Release' were found in the '/data/processed/' folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(files)} files to process.\")\n",
    "    all_dfs = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Read the CSV without a header first to inspect it\n",
    "            df_temp = pd.read_csv(file, header=None)\n",
    "            \n",
    "            # Find the actual header row by looking for keywords\n",
    "            header_row_index = -1\n",
    "            for i, row in df_temp.iterrows():\n",
    "                row_string = ' '.join(str(x) for x in row.values)\n",
    "                if \"Security\" in row_string and \"Amount\" in row_string and \"Maturity\" in row_string:\n",
    "                    header_row_index = i\n",
    "                    break\n",
    "            \n",
    "            if header_row_index != -1:\n",
    "                # Re-read the CSV, starting from the correct header row\n",
    "                df = pd.read_csv(file, header=header_row_index)\n",
    "                df['source_file'] = os.path.basename(file)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"Successfully read and processed table from {file}\")\n",
    "            else:\n",
    "                print(f\"--> Warning: Could not find a suitable header row in {file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read file: {file}. Error: {e}\")\n",
    "\n",
    "    # Consolidate and save if any data was successfully read\n",
    "    if all_dfs:\n",
    "        consolidated_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Drop any rows that are completely empty\n",
    "        consolidated_df.dropna(how='all', inplace=True)\n",
    "        \n",
    "        # Final cleanup of column names\n",
    "        consolidated_df.columns = [\n",
    "            str(col).replace('\\n', ' ').strip().lower().replace(' ', '_').replace('-(₹_crore)', '_in_cr')\n",
    "            for col in consolidated_df.columns\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n--- Preview of Consolidated Data ---\")\n",
    "        print(consolidated_df.head())\n",
    "        \n",
    "        output_path = '../data/processed/consolidated_gsec_auctions_final.csv'\n",
    "        consolidated_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSuccessfully saved consolidated data to {output_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo dataframes were created to consolidate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
